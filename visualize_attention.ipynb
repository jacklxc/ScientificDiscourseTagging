{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from util import read_passages, evaluate, make_folds, clean_words, test_f1, to_BIO, from_BIO, from_BIO_ind, arg2param\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "import keras.backend as K\n",
    "K.set_session(sess)\n",
    "from keras.activations import softmax\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping,LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from crf import CRF\n",
    "from attention import TensorAttention\n",
    "from custom_layers import HigherOrderTimeDistributedDense\n",
    "from generator import BertDiscourseGenerator\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "from discourse_tagger_generator_bert import PassageTagger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from matplotlib import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = True\n",
    "att_context = \"LSTM_clause\"\n",
    "bidirectional = bid = True\n",
    "crf = True\n",
    "lstm = False\n",
    "maxseqlen = 40\n",
    "maxclauselen = 60\n",
    "input_size = 768\n",
    "embedding_dropout=0.4 \n",
    "high_dense_dropout=0.4\n",
    "attention_dropout=0.6\n",
    "lstm_dropout=0.5\n",
    "word_proj_dim=300 \n",
    "hard_k=0 \n",
    "lstm_dim = 350 \n",
    "rec_hid_dim = 75 \n",
    "att_proj_dim = 200 \n",
    "batch_size = 10\n",
    "reg=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"scidt_scibert/\"\n",
    "model_ext = \"att=%s_cont=%s_lstm=%s_bi=%s_crf=%s\"%(str(use_attention), att_context, str(lstm), str(bid), str(crf))\n",
    "model_config_file = open(prefix+\"model_%s_config.json\"%model_ext, \"r\")\n",
    "model_weights_file_name = prefix+\"model_%s_weights\"%model_ext\n",
    "model_label_ind = prefix+\"model_%s_label_ind.json\"%model_ext\n",
    "label_ind_json = json.load(open(model_label_ind))\n",
    "label_ind = {k: int(label_ind_json[k]) for k in label_ind_json}\n",
    "num_classes = len(label_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_attention:\n",
    "    inputs = Input(shape=(maxseqlen, maxclauselen, input_size))\n",
    "    x = Dropout(embedding_dropout)(inputs)\n",
    "    x = HigherOrderTimeDistributedDense(input_dim=input_size, output_dim=word_proj_dim, reg=reg)(x)\n",
    "    att_input_shape = (maxseqlen, maxclauselen, word_proj_dim)\n",
    "    x = Dropout(high_dense_dropout)(x)\n",
    "    x, raw_attention = TensorAttention(att_input_shape, context=att_context, hard_k=hard_k, proj_dim = att_proj_dim, rec_hid_dim = rec_hid_dim, return_attention=True)(x)\n",
    "    x = Dropout(attention_dropout)(x)\n",
    "else:\n",
    "    inputs = Input(shape=(maxseqlen, input_size))\n",
    "    x = Dropout(embedding_dropout)(inputs)\n",
    "    x = TimeDistributed(Dense(input_dim=input_size, units=word_proj_dim))\n",
    "\n",
    "if bidirectional:\n",
    "    x = Bidirectional(LSTM(input_shape=(maxseqlen,word_proj_dim), units=lstm_dim, \n",
    "                                  return_sequences=True,kernel_regularizer=l2(reg),\n",
    "                                  recurrent_regularizer=l2(reg), \n",
    "                                  bias_regularizer=l2(reg)))(x)\n",
    "    x = Dropout(lstm_dropout)(x) \n",
    "elif lstm:\n",
    "    x = LSTM(input_shape=(maxseqlen,word_proj_dim), units=lstm_dim, return_sequences=True,\n",
    "                    kernel_regularizer=l2(reg),\n",
    "                    recurrent_regularizer=l2(reg), \n",
    "                    bias_regularizer=l2(reg))(x)\n",
    "    x = Dropout(lstm_dropout)(x) \n",
    "\n",
    "if crf:\n",
    "    Crf = CRF(num_classes,learn_mode=\"join\")\n",
    "    discourse_prediction = Crf(x)\n",
    "    tagger = Model(inputs=inputs, outputs=[discourse_prediction])        \n",
    "else:\n",
    "    discourse_prediction = TimeDistributed(Dense(num_classes, activation='softmax'),name='discourse')(x)\n",
    "    tagger = Model(inputs=inputs, outputs=[discourse_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.load_weights(model_weights_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crf:\n",
    "    tagger.compile(optimizer=Adam(), loss=Crf.loss_function, metrics=[Crf.accuracy])\n",
    "else:\n",
    "    tagger.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tagger.input\n",
    "attention_output = tagger.layers[4].output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functor = K.function([inp, K.learning_phase()], [attention_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"lucky_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"repfile\":\"/nas/home/xiangcil/scibert_scivocab_uncased\",\n",
    "    \"use_attention\": True,\n",
    "    \"batch_size\": 10,\n",
    "    \"maxseqlen\": 40,\n",
    "    \"maxclauselen\": 60\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = params[\"repfile\"]\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "bert = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
    "bert._make_predict_function() # Crucial step, otherwise TF will give error.\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "tokenizer = Tokenizer(token_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_seqs, label_seqs = read_passages(test_file, is_labeled=True)\n",
    "str_seqs = clean_words(str_seqs)\n",
    "label_seqs = to_BIO(label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_generator = BertDiscourseGenerator(bert, tokenizer, str_seqs, label_seqs, label_ind, 10, True, 40, 60, True, input_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y = bert_generator.make_data(str_seqs, label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_raw_scores = functor([test_X])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = softmax(attention_raw_scores,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_token_dict = {v:k for k,v in token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_seqs_tokenized = []\n",
    "for str_seq in str_seqs:\n",
    "    str_seq_tokenized = []\n",
    "    for clause in str_seq:\n",
    "        clause_tokenized = []\n",
    "        indices, segments = tokenizer.encode(clause.lower(), max_len=512)\n",
    "        for i in range(60):\n",
    "            clause_tokenized.append(reverse_token_dict[indices[i]])\n",
    "        str_seq_tokenized.append(clause_tokenized)\n",
    "    str_seqs_tokenized.append(str_seq_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(attention_raw_scores[0,:,:],cmap='gray',aspect=\"equal\",interpolation='none')\n",
    "plt.ylabel(\"discourse ID\")\n",
    "plt.xlabel(\"word ID\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_string = str_seqs_tokenized[5][-5][1:10]\n",
    "goal_weight = attention_raw_scores[5,-5,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# goal\n",
    "for token, weight in zip(str_seqs_tokenized[5][-5], attention_scores[5,-5,:]):\n",
    "    print(token, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_string = str_seqs_tokenized[5][-4][1:12]\n",
    "method_weight = attention_raw_scores[5,-4,1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# method\n",
    "for token, weight in zip(str_seqs_tokenized[5][-4], attention_scores[5,-4,:]):\n",
    "    print(token, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_string = str_seqs_tokenized[5][-3][1:14]\n",
    "results_weight = attention_raw_scores[5,-3,1:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results\n",
    "for token, weight in zip(str_seqs_tokenized[5][-3], attention_scores[5,-3,:]):\n",
    "    print(token, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_seqs = from_BIO(label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (str_seq, label_seq) in enumerate(zip(str_seqs, label_seqs)):\n",
    "    print(i,np.mean([len(clause.split()) for clause in str_seq]), len(set(label_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    low = np.min(array)\n",
    "    high = np.max(array)\n",
    "    return (array - low) / (high - low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_text(x, y, strings, alphas, orientation='horizontal',\n",
    "                 ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.axis('off')\n",
    "    t = ax.transData\n",
    "    canvas = ax.figure.canvas\n",
    "\n",
    "    assert orientation in ['horizontal', 'vertical']\n",
    "    if orientation == 'vertical':\n",
    "        kwargs.update(rotation=90, verticalalignment='bottom')\n",
    "\n",
    "    for s, alpha in zip(strings, alphas):\n",
    "        text = ax.text(x, y, s + \" \", alpha=alpha, transform=t, **kwargs)\n",
    "\n",
    "        # Need to draw to update the text position.\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "        if orientation == 'horizontal':\n",
    "            t = transforms.offset_copy(\n",
    "                text.get_transform(), x=ex.width, units='dots')\n",
    "        else:\n",
    "            t = transforms.offset_copy(\n",
    "                text.get_transform(), y=ex.height, units='dots')\n",
    "\n",
    "\n",
    "words = \"all unicorns poop rainbows ! ! !\".split()\n",
    "alphas = np.arange(1,9) * 0.1\n",
    "fig = plt.figure(figsize=(6, 0.3))\n",
    "attention_text(0, 3, goal_string, normalize(goal_weight), size=18)\n",
    "attention_text(0, 1.5, method_string, normalize(method_weight), size=18)\n",
    "attention_text(0, 0, results_string, normalize(results_weight), size=18)\n",
    "#plt.show()\n",
    "filename = \"attention_visualization.pdf\"\n",
    "plt.savefig(filename,quality=100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
