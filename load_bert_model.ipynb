{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from util import read_passages, evaluate, make_folds, clean_words, test_f1, to_BIO, from_BIO, from_BIO_ind, arg2param\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "import keras.backend as K\n",
    "K.set_session(sess)\n",
    "from keras.activations import softmax\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping,LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from crf import CRF\n",
    "from attention import TensorAttention\n",
    "from custom_layers import HigherOrderTimeDistributedDense\n",
    "from generator import BertDiscourseGenerator\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "from discourse_tagger_generator_bert import PassageTagger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from matplotlib import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = True\n",
    "att_context = \"LSTM_clause\"\n",
    "bidirectional = bid = True\n",
    "crf = True\n",
    "lstm = False\n",
    "maxseqlen = 40\n",
    "maxclauselen = 60\n",
    "input_size = 768\n",
    "embedding_dropout=0.4 \n",
    "high_dense_dropout=0.4\n",
    "attention_dropout=0.6\n",
    "lstm_dropout=0.5\n",
    "word_proj_dim=300 \n",
    "hard_k=0 \n",
    "lstm_dim = 350 \n",
    "rec_hid_dim = 75 \n",
    "att_proj_dim = 200 \n",
    "batch_size = 10\n",
    "reg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"repfile\":\"/nas/home/xiangcil/scibert_scivocab_uncased\",\n",
    "    \"use_attention\": True,\n",
    "    \"batch_size\": 10,\n",
    "    \"maxseqlen\": 40,\n",
    "    \"maxclauselen\": 60\n",
    "         }\n",
    "test_file = \"test_rct.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"LSTM_clause\"\n",
    "model_ext = \"att=%s_cont=%s_lstm=%s_bi=%s_crf=%s\"%(str(use_attention), att_context, str(lstm), str(bid), str(crf))\n",
    "model_config_file = open(prefix+\"_config.json\", \"r\")\n",
    "model_weights_file_name = prefix+\"_weights.best.hdf5\"\n",
    "model_label_ind = prefix+\"_label_ind.json\"\n",
    "label_ind_json = json.load(open(model_label_ind))\n",
    "label_ind = {k: int(label_ind_json[k]) for k in label_ind_json}\n",
    "num_classes = len(label_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnt = PassageTagger(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnt.tagger = model_from_json(model_config_file.read(), custom_objects={\"TensorAttention\":TensorAttention, \n",
    "    \"HigherOrderTimeDistributedDense\":HigherOrderTimeDistributedDense,\"CRF\":CRF})\n",
    "nnt.tagger.load_weights(model_weights_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data\n"
     ]
    }
   ],
   "source": [
    "test_seq_lengths, test_generator = nnt.make_data(test_file, label_ind=label_ind, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs, pred_label_seqs, _ = nnt.predict(test_generator, test_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"test_RCT_BioBERT_LSTM_clause.out\",\"w\")\n",
    "pred_label_seqs = from_BIO(pred_label_seqs)\n",
    "for pred_label_seq in pred_label_seqs:\n",
    "    for pred_label in pred_label_seq:\n",
    "        print(pred_label,file = outfile)\n",
    "    print(\"\",file = outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
