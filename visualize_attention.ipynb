{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from util import read_passages, evaluate, make_folds, clean_words, test_f1, to_BIO, from_BIO, from_BIO_ind, arg2param\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "import keras.backend as K\n",
    "K.set_session(sess)\n",
    "from keras.activations import softmax\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping,LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from crf import CRF\n",
    "from attention import TensorAttention\n",
    "from custom_layers import HigherOrderTimeDistributedDense\n",
    "from generator import BertDiscourseGenerator\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "from discourse_tagger_generator_bert import PassageTagger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from matplotlib import transforms\n",
    "from operator import itemgetter, attrgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = True\n",
    "att_context = \"LSTM_clause\"\n",
    "bidirectional = bid = True\n",
    "crf = True\n",
    "lstm = False\n",
    "maxseqlen = 40\n",
    "maxclauselen = 60\n",
    "input_size = 768\n",
    "embedding_dropout=0.4 \n",
    "high_dense_dropout=0.4\n",
    "attention_dropout=0.6\n",
    "lstm_dropout=0.5\n",
    "word_proj_dim=300 \n",
    "hard_k=0 \n",
    "lstm_dim = 350 \n",
    "rec_hid_dim = 75 \n",
    "att_proj_dim = 200 \n",
    "batch_size = 10\n",
    "reg=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"scidt_scibert/\"\n",
    "model_ext = \"att=%s_cont=%s_lstm=%s_bi=%s_crf=%s\"%(str(use_attention), att_context, str(lstm), str(bid), str(crf))\n",
    "model_config_file = open(prefix+\"model_%s_config.json\"%model_ext, \"r\")\n",
    "model_weights_file_name = prefix+\"model_%s_weights\"%model_ext\n",
    "model_label_ind = prefix+\"model_%s_label_ind.json\"%model_ext\n",
    "label_ind_json = json.load(open(model_label_ind))\n",
    "label_ind = {k: int(label_ind_json[k]) for k in label_ind_json}\n",
    "num_classes = len(label_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_attention:\n",
    "    inputs = Input(shape=(maxseqlen, maxclauselen, input_size))\n",
    "    x = Dropout(embedding_dropout)(inputs)\n",
    "    x = HigherOrderTimeDistributedDense(input_dim=input_size, output_dim=word_proj_dim, reg=reg)(x)\n",
    "    att_input_shape = (maxseqlen, maxclauselen, word_proj_dim)\n",
    "    x = Dropout(high_dense_dropout)(x)\n",
    "    x, raw_attention = TensorAttention(att_input_shape, context=att_context, hard_k=hard_k, proj_dim = att_proj_dim, rec_hid_dim = rec_hid_dim, return_attention=True)(x)\n",
    "    x = Dropout(attention_dropout)(x)\n",
    "else:\n",
    "    inputs = Input(shape=(maxseqlen, input_size))\n",
    "    x = Dropout(embedding_dropout)(inputs)\n",
    "    x = TimeDistributed(Dense(input_dim=input_size, units=word_proj_dim))\n",
    "\n",
    "if bidirectional:\n",
    "    x = Bidirectional(LSTM(input_shape=(maxseqlen,word_proj_dim), units=lstm_dim, \n",
    "                                  return_sequences=True,kernel_regularizer=l2(reg),\n",
    "                                  recurrent_regularizer=l2(reg), \n",
    "                                  bias_regularizer=l2(reg)))(x)\n",
    "    x = Dropout(lstm_dropout)(x) \n",
    "elif lstm:\n",
    "    x = LSTM(input_shape=(maxseqlen,word_proj_dim), units=lstm_dim, return_sequences=True,\n",
    "                    kernel_regularizer=l2(reg),\n",
    "                    recurrent_regularizer=l2(reg), \n",
    "                    bias_regularizer=l2(reg))(x)\n",
    "    x = Dropout(lstm_dropout)(x) \n",
    "\n",
    "if crf:\n",
    "    Crf = CRF(num_classes,learn_mode=\"join\")\n",
    "    discourse_prediction = Crf(x)\n",
    "    tagger = Model(inputs=inputs, outputs=[discourse_prediction])        \n",
    "else:\n",
    "    discourse_prediction = TimeDistributed(Dense(num_classes, activation='softmax'),name='discourse')(x)\n",
    "    tagger = Model(inputs=inputs, outputs=[discourse_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.load_weights(model_weights_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crf:\n",
    "    tagger.compile(optimizer=Adam(), loss=Crf.loss_function, metrics=[Crf.accuracy])\n",
    "else:\n",
    "    tagger.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tagger.input\n",
    "attention_output = tagger.layers[4].output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "functor = K.function([inp, K.learning_phase()], [attention_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"lucky_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"repfile\":\"/nas/home/xiangcil/scibert_scivocab_uncased\",\n",
    "    \"use_attention\": True,\n",
    "    \"batch_size\": 10,\n",
    "    \"maxseqlen\": 40,\n",
    "    \"maxclauselen\": 60\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = params[\"repfile\"]\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "bert = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
    "bert._make_predict_function() # Crucial step, otherwise TF will give error.\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "tokenizer = Tokenizer(token_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_seqs, label_seqs = read_passages(test_file, is_labeled=True)\n",
    "str_seqs = clean_words(str_seqs)\n",
    "label_seqs = to_BIO(label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_generator = BertDiscourseGenerator(bert, tokenizer, str_seqs, label_seqs, label_ind, 10, True, 40, 60, True, input_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y = bert_generator.make_data(str_seqs, label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_raw_scores = functor([test_X])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = softmax(attention_raw_scores,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_token_dict = {v:k for k,v in token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_seqs_tokenized = []\n",
    "for str_seq in str_seqs:\n",
    "    str_seq_tokenized = []\n",
    "    for clause in str_seq:\n",
    "        clause_tokenized = []\n",
    "        indices, segments = tokenizer.encode(clause.lower(), max_len=512)\n",
    "        for i in range(60):\n",
    "            clause_tokenized.append(reverse_token_dict[indices[i]])\n",
    "        str_seq_tokenized.append(clause_tokenized)\n",
    "    str_seqs_tokenized.append(str_seq_tokenized)\n",
    "label_seqs = from_BIO(label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_label = list(set(label.split(\"_\")[-1] for label in label_ind))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distributions = {label:{} for label in original_label}\n",
    "for para, (str_seq_tokenized, label_seq) in enumerate(zip(str_seqs_tokenized, label_seqs)):\n",
    "    for sent, (sentence, label) in enumerate(zip(str_seq_tokenized, label_seq)):\n",
    "        for idx, token in enumerate(sentence):\n",
    "            weight = attention_raw_scores[para,-len(str_seq_tokenized)+sent,idx]\n",
    "            if token[0]!=\"[\" and weight>=2.5:\n",
    "                token_weights = all_distributions[label].get(token,[])\n",
    "                token_weights.append(weight)\n",
    "                all_distributions[label][token] = token_weights\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mean_distribution = {}\n",
    "for label, token_weights in all_distributions.items():\n",
    "    this_label = []\n",
    "    for token, weights in token_weights.items():\n",
    "        count = len(weights)\n",
    "        this_label.append((token,count))\n",
    "    sorted_list = sorted(this_label, key=itemgetter(1), reverse=True)\n",
    "    all_mean_distribution[label] = sorted_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': [('we', 65), ('to', 49), ('for', 33), ('was', 25), ('were', 24)],\n",
       " 'fact': [('to', 10), ('not', 5), ('that', 4), ('well', 3), ('evidence', 3)],\n",
       " 'hypothesis': [('be', 32),\n",
       "  ('to', 23),\n",
       "  ('could', 23),\n",
       "  ('might', 22),\n",
       "  ('that', 19)],\n",
       " 'result': [('not', 102),\n",
       "  ('was', 87),\n",
       "  ('that', 48),\n",
       "  ('did', 42),\n",
       "  ('found', 39)],\n",
       " 'problem': [('not', 25),\n",
       "  ('to', 15),\n",
       "  ('however', 12),\n",
       "  ('still', 11),\n",
       "  ('been', 7)],\n",
       " 'implication': [('that', 72),\n",
       "  ('not', 20),\n",
       "  ('suggest', 13),\n",
       "  ('be', 12),\n",
       "  ('may', 10)],\n",
       " 'goal': [('to', 134),\n",
       "  ('whether', 16),\n",
       "  ('determine', 14),\n",
       "  ('investigate', 11),\n",
       "  ('we', 10)],\n",
       " 'none': [('is', 2), ('described', 2), ('shown', 1)]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mean_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
